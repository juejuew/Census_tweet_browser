{
 "cells": [
  {
   "cell_type": "raw",
   "id": "varied-court",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Calculating distances between Census tweets and respondant quote\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modified-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # word-doc matrix\n",
    "from scipy.sparse import coo_matrix # sparse matrices\n",
    "from scipy.sparse import csr_matrix # sparse row matrix\n",
    "\n",
    "# for pre-processing\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import math # log for word distances\n",
    "\n",
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LSA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\b\\w{3,}\\b')\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sufficient-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### Functions\n",
    "#############\n",
    "\n",
    "# pre-processing of tweets\n",
    "def preProcessingFcn(tweet, removeWords=list(), stem=True, removeURL=True, removeStopwords=True, \n",
    "    removeNumbers=False, removePunctuation=True):\n",
    "    \"\"\"\n",
    "    Cleans tweets by removing words, stemming, etc.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"\\\\n\", \" \", tweet)\n",
    "    tweet = re.sub(r\"&amp\", \" \", tweet)\n",
    "    if removeURL==True:\n",
    "        tweet = re.sub(r\"http\\S+\", \" \", tweet)\n",
    "    if removeNumbers==True:\n",
    "        tweet=  ''.join(i for i in tweet if not i.isdigit())\n",
    "    if removePunctuation==True:\n",
    "        for punct in string.punctuation:\n",
    "            tweet = tweet.replace(punct, ' ')\n",
    "    if removeStopwords==True:\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stopwords.words('english')])\n",
    "    if len(removeWords)>0:\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in removeWords])\n",
    "    if stem==True:\n",
    "        tweet = ' '.join([ps.stem(word) for word in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "# create word-document matrix\n",
    "def make_wordDocMatrix(cleanedTweets, minMentions = 5):\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode', min_df=minMentions, binary=True)\n",
    "    docWordMatrix = vectorizer.fit_transform(cleanedTweets)\n",
    "    colWords = vectorizer.get_feature_names()\n",
    "    output = dict()\n",
    "    output['docWordMatrix'] = docWordMatrix\n",
    "    output['words'] = colWords\n",
    "    return output\n",
    "\n",
    "# create word-distance matrix\n",
    "def get_wordDists(w, colWords, quote_cleaned):\n",
    "    # word distances\n",
    "    v = w.shape[1]\n",
    "    n = w.shape[0]\n",
    "    word_dists = []\n",
    "    for word in list(set(quote_cleaned.split())):\n",
    "        if word in colWords:\n",
    "            # column of doc-word matrix associated with quote word\n",
    "            quoteword_col = w[:,colWords.index(word)]\n",
    "            quoteword_mat = quoteword_col * coo_matrix(np.ones([1, v]))\n",
    "            multiplied = quoteword_mat.multiply(w)\n",
    "            mult_cols = multiplied.sum(axis=0).tolist()[0]\n",
    "            quote_sums = quoteword_mat.sum(axis=0).tolist()[0]\n",
    "            w_sums = w.sum(axis=0).tolist()[0]\n",
    "            add_cols = [quote_sums[i]+w_sums[i]-mult_cols[i] for i in range(v)]\n",
    "            word_dists.append([1-mult_cols[i]/add_cols[i] if add_cols[i]>0 else 1 for i in range(v)])\n",
    "            #word_dists.append([-1*math.log(mult_cols[i]/add_cols[i]+.001) if add_cols[i]>0 else -1*log(.001) for i in range(v)])\n",
    "    word_dists_matrix = np.vstack(word_dists)\n",
    "    # tweet distances\n",
    "    tweetDists = []\n",
    "    quoteWords = [colWords.index(word) for word in list(set(quote_cleaned.split())) if word in colWords]\n",
    "    for i in range(n):\n",
    "        iWords = list(np.where(w[i,:].toarray()[0]>0)[0])\n",
    "        if len(iWords)==0:\n",
    "            # if tweet i contains no words in word-doc matrix --> cannot compute distance\n",
    "            tweet_dist = 1\n",
    "        else:\n",
    "            iquoteIntersect = list(set(iWords) & set(quoteWords))\n",
    "            dtweetsij = word_dists_matrix[:, iWords]\n",
    "            entries = dtweetsij.shape[0] * dtweetsij.shape[1]\n",
    "            tweet_dist = np.sum(dtweetsij)/entries\n",
    "            if len(iquoteIntersect)>0:\n",
    "                dtweetsij_intersect = word_dists_matrix[:,iquoteIntersect]\n",
    "                dtweetsij_intersect = dtweetsij_intersect[[quoteWords.index(j) for j in iquoteIntersect], :]\n",
    "                tweet_dist = tweet_dist - np.sum(dtweetsij_intersect)/entries\n",
    "        tweetDists.append(tweet_dist)\n",
    "    return tweetDists\n",
    "    \n",
    "\n",
    "# LDA: new 'docWordMatrix' for quote\n",
    "def quote_docWordMat(quote_cleaned, colWords):\n",
    "    cols = []\n",
    "    data = []\n",
    "    for word in list(set(quote_cleaned.split())):\n",
    "        if word in colWords:\n",
    "            cols.append(colWords.index(word))\n",
    "            data.append(quote_cleaned.split().count(word))\n",
    "    new_docWordMat = csr_matrix((np.array(data), (np.repeat(0, len(cols)), np.array(cols))), \n",
    "                                shape=(1, len(colWords)))\n",
    "    return new_docWordMat\n",
    "\n",
    "def LDA_dists(quote_distribution, tweet_distributions, method='Euclidean'):\n",
    "    distances = []\n",
    "    for tweet_distribution in tweet_distributions:\n",
    "        if method=='Euclidean':\n",
    "            distances.append(np.linalg.norm(quote_distribution-tweet_distribution))\n",
    "    return distances\n",
    "\n",
    "\n",
    "# LSA\n",
    "def train_LSA(tweets, n_components=20):\n",
    "    tfidf = TfidfVectorizer(lowercase=True,\n",
    "                            tokenizer=tokenizer.tokenize,\n",
    "                            max_df=.2)\n",
    "    tfidf_train_sparse = tfidf.fit_transform(tweets)\n",
    "    tfidf_train_df = pd.DataFrame(tfidf_train_sparse.toarray(), \n",
    "                        columns=tfidf.get_feature_names())\n",
    "    lsa_obj = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    lsa_obj.fit(tfidf_train_df)\n",
    "    tfidf_lsa_data = lsa_obj.transform(tfidf_train_df)\n",
    "    output = dict()\n",
    "    output['tfidf_lsa_data'] = tfidf_lsa_data\n",
    "    output['tfidf'] = tfidf\n",
    "    output['lsa_obj'] = lsa_obj\n",
    "    return output\n",
    "\n",
    "def quote_LSA(quote_cleaned, tfidf, lsa_obj):\n",
    "    tfidf_quote = tfidf.transform([quote_cleaned])\n",
    "    lsa_quote = lsa_obj.transform(tfidf_quote)\n",
    "    return lsa_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-colors",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-sheriff",
   "metadata": {},
   "source": [
    "#### - Sample of already cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (30,31,41,45,50,70) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        realdailywir new censu data suggest previou re...\n",
       "1        project show florida north carolina among stat...\n",
       "2        catstalkback1 sometim truth hard swallow still...\n",
       "3        wwg1wga ww shame aoc cld lose hous seat 2020 e...\n",
       "4                     excit thing 2020 censu bitch it‚Äô lit\n",
       "                               ...                        \n",
       "17489    begood31567 colinbanks44 realericshaff depend ...\n",
       "17491    blindguy97 lol still flashback work censu bure...\n",
       "17492    read share jeopard franchis mani peopl die end...\n",
       "17494    pleas fill censu censu help fund school hospit...\n",
       "17495    naicob91 accord gwu 2015 censu respons 12 bill...\n",
       "Name: cleaned_rt, Length: 12244, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in sample of already cleaned tweets\n",
    "allMessages = pd.read_csv('allCensus_sample.csv')\n",
    "allMessages['cleaned_rt'] = [tweet[3:] if tweet[0:2]=='rt' else tweet for tweet in allMessages['cleaned']]\n",
    "tweets_cleaned = allMessages['cleaned_rt']\n",
    "tweets_cleaned = tweets_cleaned.drop_duplicates()\n",
    "tweets = tweets_cleaned\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "selected-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word-document matrix\n",
    "docWordMatrix = make_wordDocMatrix(tweets, minMentions=3)\n",
    "w = docWordMatrix['docWordMatrix']\n",
    "colWords = docWordMatrix['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personalized-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12244x6174 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 208270 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "whole-dance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000', '00am', '00pm', '01', '02', '03', '04', '041', '05']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colWords[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "illegal-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda.fit(w)\n",
    "topicDists = lda.transform(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strange-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "lsa = train_LSA(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "close-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rep/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "least-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quote\n",
    "quote ='''\n",
    "The people came around to my door and I felt like it was none of their business. \n",
    "What goes on in my house is totally private. The government shouldn‚Äôt care. \n",
    "I don‚Äôt think they deserve to know what‚Äôs going on in my home. \n",
    "They don‚Äôt do anything to help me, why should I answer any questions for them?\n",
    "'''\n",
    "\n",
    "quote = '''\n",
    "[Census information is shared] with the entire government. With everyone in the government‚Ä¶\n",
    "police, immigration, hospitals, everything, everything, everything. Everything is connected.\n",
    "'''\n",
    "\n",
    "quote = '''\n",
    "The government has always been intrusive as it is, and it‚Äôs probably a level of intrusion. \n",
    "That‚Äôs why people are like, ‚ÄòHold on, what you want to know what‚Äôs in my bed, \n",
    "at my house, and who‚Äôs using my toilet? You should go mind your business.\n",
    "'''\n",
    "\n",
    "quote = '''\n",
    "[Latinos will not participate] out of fear‚Ä¶[there] is practically a hunt [for us] ‚Ä¶\n",
    "and many of us Latinos are going to be afraid to be counted because of the \n",
    "retaliation that could happen because it's like giving the government information,\n",
    " of saying, ‚ÄòOh, there are more here.\n",
    " '''\n",
    " \n",
    "quote = '''\n",
    " I think it‚Äôs a necessity. I think the immigrants need to get out of here, \n",
    " you know what I mean. I mean, even with a green card‚ÄîI don‚Äôt agree with it. \n",
    " I just don‚Äôt.\n",
    " '''\n",
    "\n",
    "\n",
    "quote_cleaned = preProcessingFcn(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-anger",
   "metadata": {},
   "source": [
    "#### (1) Ferg's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conscious-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet distances\n",
    "tweetDists = get_wordDists(w, colWords, quote_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "commercial-night",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12244"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweetDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "macro-fabric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"RT @denlusk13 We can't even get illegals counted on the census!\",\n",
       " \"RT @PopGeog Here's what you need to know about the 2020 Census abc7.com/community-even‚Ä¶ via @abc7\",\n",
       " \"RT @ABOwarrior Anybody else receive a call as a follow up to census to get more information? I didn't agree to answer the questions. Said it would take seven minutes. I said how do I know you are who you say you are? If you need more information Inquire through the mail. They said they couldn't\",\n",
       " \"RT @CACensus Happy Mardi Gras! We're just a few weeks away from the start of the #2020Census which means we're even closer to getting funding for resources our families need. Visit californiacensus.org to learn more about how to ensure your community is counted. #ICount #CaliforniaForAll https://t.co/LoWCvydufA\",\n",
       " \"@AveryBellCampb1 But shouldn't they know my ID before mailing my ballot? They don't need ID for the census\",\n",
       " '@SonderWander2 @CharlieDaniels We need a census!!!!',\n",
       " 'What you need to know before 2020 census starts in Alaska ‚Äì\\xa0NEWPAPER24 newpaper24.com/what-you-need-‚Ä¶ https://t.co/slrVJkMyau',\n",
       " 'RT @OtherJoeBiden I told Mandela in the prison that we needed a national cancer- I mean census of the disaster of the whole block, street. You know the thing. How do you align most?',\n",
       " \"RT @swingleft Fill out your census today! An accurate count means your community will get the representation and funding it needs. Take a few minutes now and let's do it. #CensusDay 2020census.gov\",\n",
       " 'RT @uscensusbureau Did you know that even if you rent your home you need to respond to the #2020Census? Complete the 2020 Census today. Learn more at 2020CENSUS.GOV. pic.twitter.com/f74kGSBk5G']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get closest n tweets\n",
    "n_min_values = 10\n",
    "rowValues = sorted(range(len(tweetDists)), key=lambda k: tweetDists[k])[:n_min_values] \n",
    "indexValues = tweets_cleaned.index[rowValues]\n",
    "\n",
    "list(allMessages['Message'].loc[indexValues])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-aside",
   "metadata": {},
   "source": [
    "#### (2) LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clinical-dodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00416667, 0.00416667, 0.00416667, 0.00416667, 0.00416667,\n",
       "        0.00416667, 0.00416667, 0.00416667, 0.00416667, 0.83497483,\n",
       "        0.00416667, 0.00416667, 0.00416667, 0.00416667, 0.00416667,\n",
       "        0.00416667, 0.00416667, 0.00416667, 0.09002517, 0.00416667]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_w = quote_docWordMat(quote_cleaned, colWords)\n",
    "quoteDist = lda.transform(quote_w)\n",
    "quoteDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "demanding-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances\n",
    "dists = LDA_dists(quoteDist[0], topicDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sunrise-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Republican Party sends deceptive fundraising mailers that look like census just weeks ahead of official forms go out hill.cm/hRVPI4N https://t.co/E3H5a8qu1o',\n",
       " 'Will someone setup a website requiring driver‚Äôs license photo or scanning of license to register in order to get a mail out ballot? @realDonaldTrump The address on your license must match your ballot destination... @greggutfeld @JesseBWatters Also, this would help Census',\n",
       " 'RT @Smilynntodd üíôKatie Porter‚Äôs not having it!\\r\\n\\r\\nCensus Director: ‚ÄúI‚Äôll have to see. I‚Äôm not sure who sent that out.‚Äù\\r\\n\\r\\nPorter: ‚ÄúI am!‚Äù This isn‚Äôt the first time we‚Äôve seen the RNC try to confuse voters by sending them a mailer that imitates the census. Have you asked the RNC to cease and desist! https://t.co/8KwalEX36v',\n",
       " '@JimCelania It‚Äôs more the tax return than the census I fear üò±',\n",
       " '@charlieworsham @SadlerVaden Glad everyone finally came to their census.',\n",
       " 'So we filling out census forms during the time of a pandemic?',\n",
       " 'RT @KyleIM I need to see this same vote energy for the census.',\n",
       " 'FAKE FAKE FAKE DO NO FILL OUT THIS FORM! IT IS NOT TRUE CENSUS!!',\n",
       " 'RT @ridgemadison @RepAOC #Democrats should be screaming this from the rooftops, not talking about the friggin‚Äô census. #wherearethedemocrats?\\r\\n\\r\\nTHIS üëáüèªüëáüèªüëáüèª https://t.co/4oHeAinI9u',\n",
       " 'Do you think Census and Kentos would want to hang out?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get closest n tweets\n",
    "n_min_values = 10\n",
    "rowValues = sorted(range(len(dists)), key=lambda k: dists[k])[:n_min_values]\n",
    "indexValues = tweets_cleaned.index[rowValues]\n",
    "\n",
    "list(allMessages['Message'].loc[indexValues])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-particle",
   "metadata": {},
   "source": [
    "#### (3) LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "preliminary-customer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09125403, -0.03435152, -0.09682721,  0.03765667,  0.00829608,\n",
       "        -0.02042731,  0.04926003, -0.0199747 , -0.0138575 , -0.05594865,\n",
       "         0.01678145, -0.00477414,  0.07995748, -0.06754421,  0.01117948,\n",
       "         0.0482084 ,  0.03031363, -0.007011  , -0.07910353,  0.01871058]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit LSA to quote\n",
    "lsa_quote_dist = quote_LSA(quote_cleaned, lsa['tfidf'], lsa['lsa_obj'])\n",
    "lsa_quote_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "anticipated-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distances\n",
    "dists_LSA = LDA_dists(lsa_quote_dist, lsa['tfidf_lsa_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "metallic-ecology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @kimmydeenyc @BlueSteelDC @SallyAlbright There is a federally paid effort to get out the census. I know multiple people working in her district and see the ads running there. Why does she need to raise money for this?',\n",
       " '@lustrelux how did yall finally get the census people to leave you alone? Asking for myself lol',\n",
       " '@RealJamesWoods Now things get interesting.  Without the \"citizen\" question on the census, how are states going to comply?  As I understand it, this is a presidential power.. so yeah.. interesting.  This will probably go all the way to the supreme court.',\n",
       " 'RT @BernardKerik @chicagosmayor This is why democrat run cities are suffering with the most violent crime, shootings, and murder. Nobody gives a damn about census, when they‚Äôre scared to death to come out of their house for getting robbed or killed. You need to get your priorities straight.',\n",
       " \"@KOBBYANY @DTenenbaum @bethanyshondark You said yourself you need to sample 10% of the population to get an accurate representation. That is simply not true. I used the US census as an example of a sample size far below what you're talking about. Why do you need a bigger sample for a disease?\",\n",
       " 'RT @HoustoninAction Immigrants count. Young children count. LGBTQ folks count. People of color count -- #WECount & must be recorded in the #2020Census! Visit my2020census.gov to get counted.\\r\\n\\r\\nbit.ly/2YRHpeG \\r\\n\\r\\n#HTownCounts #YestotheCensus #SialCenso #IntoAction\\r\\n #SaveTheCensus. https://t.co/tMK6Z0uvvp',\n",
       " \"LGBTQ People Urged to 'Get Counted' in Census - Georgia Voice ift.tt/2UT0F8Z\",\n",
       " 'Ya know, Census takers getting PPE and providing COVID-19 tests could become the best collab of 2020.',\n",
       " \"The Census does not ask sexual orientation but it DOES ask about relationships. Same-sex married spouses and same-sex unmarried partners can check a box on the Census. We need better #LGBTQ representation. Let's get counted at my2020census.gov #CountUsIn #AAPI2020 https://t.co/BoG45GseeA\",\n",
       " 'Biden Gets Confused Again, Doesn\\'t Know What Year it is: \"In the 2020 Census, Which is Now Two Censuses Ago\" (VIDEO) thegatewaypundit.com/2020/07/biden-‚Ä¶ via @gatewaypundit']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get closest n tweets\n",
    "n_min_values = 10\n",
    "rowValues = sorted(range(len(dists_LSA)), key=lambda k: dists_LSA[k])[:n_min_values]\n",
    "indexValues = tweets_cleaned.index[rowValues]\n",
    "\n",
    "list(allMessages['Message'].loc[indexValues])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
