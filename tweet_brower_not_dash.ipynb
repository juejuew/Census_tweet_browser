{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separate-passion",
   "metadata": {},
   "source": [
    "## Tweet Browser but not in Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import chart_studio.plotly as py\n",
    "import chart_studio.tools as tls\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse.csc import csc_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# for pre-processing\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn.feature_extraction.text # tfidf\n",
    "import umap.umap_ as umap\n",
    "import textwrap # hover text on dimension reduction/clustering plot\n",
    "\n",
    "# clustering options\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "# only for spyder\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "north-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "##########   Analysis Functions   ###########################\n",
    "#############################################################\n",
    "\n",
    "# make document-word matrix from allMessages\n",
    "def make_full_docWordMatrix(allMessages_json):\n",
    "    if allMessages_json is not None:\n",
    "        # de-json-ify cleaned tweets\n",
    "        allMessages = pd.read_json(allMessages_json)\n",
    "        cleanedTweets = allMessages['cleaned']\n",
    "        # create document-word matrix\n",
    "        vectorizer = CountVectorizer(strip_accents='unicode', min_df=2, binary=False)\n",
    "        docWordMatrix_orig = vectorizer.fit_transform(cleanedTweets)\n",
    "        docWordMatrix_orig = docWordMatrix_orig.astype(dtype='float64')\n",
    "        # save as json\n",
    "        rows_orig, cols_orig = docWordMatrix_orig.nonzero()\n",
    "        data_orig = docWordMatrix_orig.data\n",
    "        docWordMatrix_orig_json = json.dumps({'rows_orig':rows_orig.tolist(), 'cols_orig':cols_orig.tolist(),\n",
    "            'data_orig':data_orig.tolist(), 'dims_orig':[docWordMatrix_orig.shape[0], docWordMatrix_orig.shape[1]],\n",
    "            'feature_names':vectorizer.get_feature_names(), 'indices':allMessages.index.tolist()})\n",
    "        return docWordMatrix_orig_json\n",
    "\n",
    "# subset word-document matrix by words\n",
    "def subset_docWordMatrix(button, docWordMatrix_orig_json, removeWords, keepWords):\n",
    "    if docWordMatrix_orig_json:\n",
    "        json_data = json.loads(docWordMatrix_orig_json)\n",
    "        data = json_data['data_orig']\n",
    "        rows = json_data['rows_orig']\n",
    "        cols = json_data['cols_orig']\n",
    "        dims = json_data['dims_orig']\n",
    "        feature_names = json_data['feature_names']\n",
    "        indices = json_data['indices']\n",
    "        docWordMatrix_update = csc_matrix((data, (rows, cols)), shape=(dims[0], dims[1]))\n",
    "\n",
    "        # subset info to output\n",
    "        subset_info_remove = ''; subset_info_keep = ''\n",
    "        remove_rowSums = [0] * docWordMatrix_update.shape[0]\n",
    "        keep_rowSums = [1] * docWordMatrix_update.shape[0]\n",
    "        if removeWords is not None: # if any remove words entered\n",
    "            if len(removeWords)>0:\n",
    "                remove_words = removeWords.split(',')\n",
    "                remove_words_cleaned = [preProcessingFcn(word) for word in remove_words]\n",
    "                remove_cols = [feature_names.index(word) for word in remove_words_cleaned if word in feature_names]\n",
    "                # restrict document-word matrix to columns of remove words and take the sum of the rows\n",
    "                remove_rowSums = docWordMatrix_update.tocsr()[:,remove_cols].sum(axis=1)\n",
    "                # add remove words to print statement\n",
    "                subset_info_remove = 'Removing tweets that contain: '\n",
    "                for i in range(len(remove_words)):\n",
    "                    subset_info_remove += remove_words[i] + '(' + remove_words_cleaned[i] + '),'\n",
    "                subset_info_remove = subset_info_remove[:-1] + '. '\n",
    "        if keepWords is not None: # if any keeps words entered\n",
    "            if len(keepWords)>0:\n",
    "                keep_words = keepWords.split(',')\n",
    "                keep_words_cleaned = [preProcessingFcn(word) for word in keep_words]\n",
    "                keep_cols = [feature_names.index(word) for word in keep_words_cleaned if word in feature_names]\n",
    "                # restrict document-word matrix to columns of keep words and take sum of the rows\n",
    "                keep_rowSums = docWordMatrix_update.tocsr()[:,keep_cols].sum(axis=1)\n",
    "                # add keep words to print statement\n",
    "                subset_info_keep = 'Keeping tweets that contain: '\n",
    "                for i in range(len(keep_words)):\n",
    "                    subset_info_keep += keep_words[i] + '(' + keep_words_cleaned[i] + '),'\n",
    "                subset_info_keep = subset_info_keep[:-1] + '.'\n",
    "        # restrict to remove/keep words\n",
    "        # new doc-word matrix (and indices) as rows where total keep words >0 and total remove words =0\n",
    "        new_rows = [i for i in range(len(remove_rowSums)) if remove_rowSums[i]==0 and keep_rowSums[i]>=1]\n",
    "        docWordMatrix_update = docWordMatrix_update.tocsr()[new_rows]\n",
    "        tfidf = sklearn.feature_extraction.text.TfidfTransformer(norm='l1').fit(docWordMatrix_update)\n",
    "        docWordMatrix_tfidf = tfidf.transform(docWordMatrix_update)\n",
    "\n",
    "        # convert to json\n",
    "        rows_new, cols_new = docWordMatrix_tfidf.nonzero()\n",
    "        data_new = docWordMatrix_tfidf.data\n",
    "        docWordMatrix_json = json.dumps({'rows_new':rows_new.tolist(), 'cols_new':cols_new.tolist(), 'data_new':data_new.tolist(),\n",
    "            'dims_new':[docWordMatrix_tfidf.shape[0], docWordMatrix_tfidf.shape[1]], 'feature_names':feature_names})\n",
    "\n",
    "        new_indices_json = json.dumps({'indices':[indices[i] for i in new_rows]})\n",
    "\n",
    "    else:\n",
    "        docWordMatrix_json = None; new_indices_json = None\n",
    "        subset_info_remove = ''; subset_info_keep = ''\n",
    "    \n",
    "    return docWordMatrix_json, subset_info_remove, subset_info_keep, new_indices_json\n",
    "\n",
    "# dimension reduction part 1: PCA\n",
    "def PCA_docWordMatrix(docWordMatrix_json):\n",
    "    if docWordMatrix_json is not None:\n",
    "        json_data = json.loads(docWordMatrix_json)\n",
    "        data = json_data['data_new']\n",
    "        rows = json_data['rows_new']\n",
    "        cols = json_data['cols_new']\n",
    "        dims = json_data['dims_new']\n",
    "        docWordMatrix = csc_matrix((data, (rows, cols)), shape=(dims[0], dims[1]))\n",
    "        tsvd = TruncatedSVD(n_components=25)\n",
    "        tsvd.fit(docWordMatrix)\n",
    "        docWordMatrix_pca = tsvd.transform(docWordMatrix)\n",
    "        docWordMatrix_pca_json = json.dumps(docWordMatrix_pca.tolist())\n",
    "        return docWordMatrix_pca_json\n",
    "\n",
    "# reduce to 2D using UMAP\n",
    "def get_dimRed_points(docWordMatrix_pca_json, new_indices_json, dimRed_method):\n",
    "    if docWordMatrix_pca_json is not None:\n",
    "        docWordMatrix_pca = pd.read_json(docWordMatrix_pca_json)\n",
    "        indices = json.loads(new_indices_json)['indices']\n",
    "        # do dimension reduction\n",
    "        if dimRed_method == 'umap':\n",
    "            umap_2d = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.0)\n",
    "            proj_2d = umap_2d.fit_transform(docWordMatrix_pca)  \n",
    "        # convert 2d points to json\n",
    "        dimRed_df = pd.DataFrame({'coord1':proj_2d[:,0], 'coord2':proj_2d[:,1]}, index=indices)\n",
    "        dimRed_points_json = dimRed_df.to_json()\n",
    "\n",
    "        return dimRed_points_json\n",
    "    \n",
    "# clustering on 2D points\n",
    "def make_cluster_plot(dimRed_points_json, allMessages_json, clustering_method, num_clusters, min_obs):\n",
    "    dimRed_cluster_plot = {}\n",
    "    if dimRed_points_json is not None:\n",
    "        # get dimension reduced points\n",
    "        dimRed_points = pd.read_json(dimRed_points_json)\n",
    "        # get allMessages\n",
    "        allMessages = pd.read_json(allMessages_json)\n",
    "        # merge\n",
    "        allMessages_plot = allMessages.merge(dimRed_points, how='right', left_index=True, right_index=True)\n",
    "        allMessages_plot['Text'] = allMessages_plot['Message'].apply(lambda t: \"<br>\".join(textwrap.wrap(t)))\n",
    "        # do clustering\n",
    "        if clustering_method=='gmm':\n",
    "            gmm = GaussianMixture(n_components=num_clusters, random_state=42).fit(allMessages_plot[['coord1', 'coord2']])\n",
    "            allMessages_plot['Cluster'] = gmm.predict(allMessages_plot[['coord1', 'coord2']]).astype(str)\n",
    "        if clustering_method=='k-means':\n",
    "            kmeans = KMeans(init='random', n_clusters=num_clusters, random_state=42)\n",
    "            allMessages_plot['Cluster'] = kmeans.fit(allMessages_plot[['coord1', 'coord2']]).labels_.astype(str)\n",
    "        if clustering_method=='hdbscan':\n",
    "            hdbscan_fcn = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=min_obs)\n",
    "            allMessages_plot['Cluster'] = hdbscan_fcn.fit_predict(allMessages_plot[['coord1', 'coord2']]).astype(str)\n",
    "        dimRed_cluster_plot = px.scatter(allMessages_plot, x='coord1', y='coord2', color='Cluster',\n",
    "            hover_data=['Text'])\n",
    "        dimRed_cluster_plot.update_layout(clickmode='event+select')\n",
    "        # cluster info to output\n",
    "        cluster_data = allMessages_plot[['Cluster']]\n",
    "        cluster_data_json = cluster_data.to_json()\n",
    "    return dimRed_cluster_plot, cluster_data_json\n",
    "\n",
    "# make table of cluster info\n",
    "def make_cluster_table(docWordMatrix_orig_json, cluster_data_json):\n",
    "    if cluster_data_json is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        # de-json-ify doc-word matrix\n",
    "        json_data = json.loads(docWordMatrix_orig_json)\n",
    "        data = json_data['data_orig']\n",
    "        rows = json_data['rows_orig']\n",
    "        cols = json_data['cols_orig']\n",
    "        dims = json_data['dims_orig']\n",
    "        feature_names = json_data['feature_names']\n",
    "        indices = json_data['indices']\n",
    "        docWordMatrix = csc_matrix((data, (rows, cols)), shape=(dims[0], dims[1]))\n",
    "        # de json-ify cluster data\n",
    "        cluster_data = pd.read_json(cluster_data_json)\n",
    "        # representative words and tweet IDs for each cluster\n",
    "        clusters_for_table = []; topWords_data = []; prop_data = []; num_data = []\n",
    "        for i in cluster_data.Cluster.unique():\n",
    "            # restrict docWordMatrix to rows in cluster i\n",
    "            i_indices = cluster_data.index[cluster_data['Cluster']==i].tolist()\n",
    "            docWordMat_rows = [j for j in range(len(indices)) if indices[j] in i_indices]\n",
    "            clusteri = docWordMatrix[docWordMat_rows, :]\n",
    "            # get most common words\n",
    "            colSumsi = np.squeeze(np.asarray(clusteri.sum(axis=0)))\n",
    "            top5 = np.sort(colSumsi)[-5]\n",
    "            topWordsi = [feature_names[j] for j in range(len(colSumsi)) if colSumsi[j]>=top5][0:5]\n",
    "            # add to display data\n",
    "            clusters_for_table.append(i)\n",
    "            topWords_data.append(\" \".join(topWordsi))\n",
    "            prop_data.append(round(clusteri.shape[0]/dims[0], 3))\n",
    "            num_data.append(clusteri.shape[0])\n",
    "        display_df = pd.DataFrame(data={'Cluster':clusters_for_table, 'Proportion of Tweets':prop_data,\n",
    "            'Number of Tweets':num_data, 'Top Stemmed Words':topWords_data})\n",
    "\n",
    "        return display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-wyoming",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "horizontal-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory with data\n",
    "#os.chdir('') # Run this line if you run the code on the host computer\n",
    "\n",
    "# dataset to use\n",
    "dataset = 'allCensus_sample.csv' # use this data set as an example\n",
    "\n",
    "# subsetting\n",
    "removeWords = None\n",
    "keepWords = None\n",
    "\n",
    "# clustering\n",
    "clustering_method = 'gmm' # options: 'hdbscan', 'gmm', 'k-means'\n",
    "num_cluster = 30 # for gmm and k-means\n",
    "min_obs = 500 # for hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-verification",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affected-vessel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3155: DtypeWarning:\n",
      "\n",
      "Columns (30,31,41,45,50,70) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "allMessages = pd.read_csv(dataset)\n",
    "if type(allMessages['UniversalMessageId'][0]) is not str:\n",
    "    allMessages['UniversalMessageId'] = ['twitter'+allMessages['UniversalMessageId'][i].astype(str) for i in range(allMessages.shape[0])]\n",
    "allMessages = allMessages.set_index('UniversalMessageId')\n",
    "allMessages = allMessages.dropna(subset=['cleaned'])\n",
    "allMessages_json = allMessages.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outstanding-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document-word matrix\n",
    "docWordMatrix_orig_json = make_full_docWordMatrix(allMessages_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "victorian-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset by words\n",
    "docWordMatrix_json, subset_info_remove, subset_info_keep, new_indices_json = subset_docWordMatrix('button', docWordMatrix_orig_json, removeWords, keepWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension\n",
    "docWordMatrix_pca_json = PCA_docWordMatrix(docWordMatrix_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broke-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 2D using UMAP\n",
    "dimRed_points_json = get_dimRed_points(docWordMatrix_pca_json, new_indices_json, dimRed_method='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naval-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering and plot\n",
    "dimRed_cluster_plot, cluster_data_json = make_cluster_plot(dimRed_points_json, allMessages_json, clustering_method, num_cluster, min_obs)\n",
    "# dimRed_cluster_plot.show() # Run this line if you run the code on the host computer; do not run this line if you run the code in the container\n",
    "dimRed_cluster_plot.write_html(\"dimRed_cluster_plot.html\") # Find this html file in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "checked-status",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cluster  Proportion of Tweets  Number of Tweets                      Top Stemmed Words\n",
      "0         1                 0.162              2836               bureau censu peopl rt us\n",
      "1         7                 0.070              1219               censu com pic rt twitter\n",
      "2         9                 0.069              1206                2020 censu com count rt\n",
      "3         4                 0.043               753          censu census2020 count gov rt\n",
      "4         0                 0.053               924              censu complet fill gov rt\n",
      "5        27                 0.012               214              censu com count counti rt\n",
      "6        10                 0.035               621            censu count immigr rt trump\n",
      "7        24                 0.103              1803           2020censu censu count gov rt\n",
      "8        19                 0.039               685              2020 censu com rt twitter\n",
      "9        15                 0.058              1020           2020censu censu count gov rt\n",
      "10       21                 0.069              1201                2020 censu count get rt\n",
      "11       26                 0.039               680                2020 censu fill form rt\n",
      "12        3                 0.037               641                2020 censu fill mail rt\n",
      "13       13                 0.013               230              censu right rt take thing\n",
      "14       22                 0.016               288                2020 censu com count rt\n",
      "15        6                 0.029               501       censu com mail rt uscensusbureau\n",
      "16       18                 0.011               186              2020 actual censu like rt\n",
      "17       12                 0.012               212              censu count form peopl rt\n",
      "18       16                 0.033               572  2020censu censu gov rt uscensusbureau\n",
      "19        5                 0.010               170               censu count get illeg rt\n",
      "20       17                 0.009               149       2020 cannot censu complet receiv\n",
      "21       20                 0.007               124          2020 censu decad help sabotag\n",
      "22       23                 0.010               179             alien censu count illeg rt\n",
      "23       25                 0.004                67               anoth attempt even it rt\n",
      "24       29                 0.006               103              censu peopl rt state unit\n",
      "25        8                 0.009               162              censu commun like rt take\n",
      "26       11                 0.004                77        censu citi coronaviru new peopl\n",
      "27       14                 0.007               122           censu citizen count illeg rt\n",
      "28       28                 0.008               137            censu court power rt suprem\n",
      "29        2                 0.024               415               big can censu count didn\n"
     ]
    }
   ],
   "source": [
    "# table of cluster info\n",
    "cluster_table = make_cluster_table(docWordMatrix_orig_json, cluster_data_json)\n",
    "print(cluster_table.to_string())\n",
    "\n",
    "text_file = open('table.html','w')\n",
    "text_file.write(cluster_table.to_html())\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
